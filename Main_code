import numpy as np
import pandas as pd
import pickle
import os
# CVAE 
# Import python package
import keras
import tensorflow as tf
import warnings
from tensorflow.keras.layers import Dense
from keras.layers import Input, Lambda
from keras.layers import concatenate as concat
from keras.models import Model
from keras import backend as K
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
# from scipy.misc import imsave
import imageio
imsave = imageio.imsave
import matplotlib.pyplot as plt
plt.rc('font',family='Times New Roman')
plt.rcParams['figure.dpi'] = 300
warnings.filterwarnings('ignore')
%pylab inline
import sklearn
from cvxopt import matrix, solvers

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

# X_train, y_train, y_trainc, X_test, y_test, y_testc

def seed_tensorflow(seed = 42):
    os.environ['PYTHONHASHSEED'] = str(seed) 
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1' # `pip install tensorflow-determinism` first,使用与tf>2.1
our_seed = 2023
seed_tensorflow(our_seed)

# Kernel Mean Matching 
def kernel(ker, X1, X2, gamma):
    K = None
    if ker == 'linear':
        if X2 is not None:
            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1), np.asarray(X2))
        else:
            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1))
    elif ker == 'rbf':
        if X2 is not None:
            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1), np.asarray(X2), gamma)
        else:
            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1), None, gamma)
    return K

class KMM:
    def __init__(self, kernel_type='linear', gamma=1.0, B=1.0, eps=None):
        '''
        Initialization function
        :param kernel_type: 'linear' | 'rbf'
        :param gamma: kernel bandwidth for rbf kernel
        :param B: bound for beta
        :param eps: bound for sigma_beta
        '''
        self.kernel_type = kernel_type
        self.gamma = gamma
        self.B = B
        self.eps = eps

    def fit(self, Xs, Xt):
        '''
        Fit source and target using KMM (compute the coefficients)
        :param Xs: ns * dim
        :param Xt: nt * dim
        :return: Coefficients (Pt / Ps) value vector (Beta in the paper)
        '''
        ns = Xs.shape[0]
        nt = Xt.shape[0]
        if self.eps == None:
            self.eps = self.B / np.sqrt(ns)
        K = kernel(self.kernel_type, Xs, None, self.gamma)
        kappa = np.sum(kernel(self.kernel_type, Xs, Xt, self.gamma) * float(ns) / float(nt), axis=1)

        K = matrix(K.astype(np.double))
        kappa = matrix(kappa.astype(np.double))
        G = matrix(np.r_[np.ones((1, ns)), -np.ones((1, ns)), np.eye(ns), -np.eye(ns)])
        h = matrix(np.r_[ns * (1 + self.eps), ns * (self.eps - 1), self.B * np.ones((ns,)), np.zeros((ns,))])

        sol = solvers.qp(K, -kappa, G, h)
        beta = np.array(sol['x'])
        return beta

# Hyper-parameters settings
n_z = 8 # latent space size
encoder_dim1 = 128 # dim of encoder hidden layer
encoder_dim2 = 64
decoder_dim1 = 64 # dim of decoder hidden layer
decoder_dim2 = 128
decoder_out_dim = 24 # dim of decoder output layer
activ = 'relu'
m = 128
learning_rate = 0.005
n_epoch = 500

n_x = X_train.shape[1]
n_y = y_train.shape[1]

# Encoder settings
X = Input(shape=(n_x,))
label = Input(shape=(n_y,))
inputs = concat([X, label])
encoder_h1 = Dense(encoder_dim1, activation=activ, name = 'encoder_first')(inputs)
encoder_h2 = Dense(encoder_dim2, activation=activ, name = 'encoder_second')(encoder_h1)
mu = Dense(n_z, activation='linear', name = 'encoder_out_mu')(encoder_h2)
l_sigma = Dense(n_z, activation='linear', name = 'encoder_out_sigma')(encoder_h2)
def sample_z(args):
    mu, l_sigma = args
    mu_num = tf.shape(mu)[0]
    eps = K.random_normal(shape=(mu_num, n_z), mean=0., stddev=1.)
    return mu + K.exp(l_sigma / 2) * eps
# Sampling latent space
z = Lambda(sample_z, output_shape = (n_z, ), name = 'lambda_layer')([mu, l_sigma])
# Merge latent space with label
zc = concat([z, label])

# Decoder settings
decoder_hidden1 = Dense(decoder_dim1, activation=activ)
decoder_hidden2 = Dense(decoder_dim2, activation=activ)
decoder_out = Dense(decoder_out_dim, activation='tanh')
h_p1 = decoder_hidden1(zc)
h_p2 = decoder_hidden2(h_p1)
outputs = decoder_out(h_p2)

# define loss
def vae_loss(y_true, y_pred):
    recon = K.sum(K.abs(y_true-y_pred), axis=-1)
    kl = 0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=-1)
    return recon + 1 * kl * kl/(kl+recon)

# def cvae_loss(y_true, y_pred):
#     recon = K.sum(K.abs(y_true-y_pred), axis=-1)
#     kl = 0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=-1)
#     return recon +  kl

def KL_loss(y_true, y_pred):
    return(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))

def recon_loss(y_true, y_pred):
    return K.sum(K.abs(y_true-y_pred), axis=-1)

# CVAE-model build
cvae = Model(inputs=[X, label], outputs = outputs)
encoder = Model(inputs=[X, label],outputs = mu)

d_in = Input(shape=(n_z+n_y,))
d_h1 = decoder_hidden1(d_in)
d_h2 = decoder_hidden2(d_h1)
d_out = decoder_out(d_h2)
decoder = Model(inputs= d_in, outputs = d_out)

class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.KL_loss = []
        self.recon_loss = []
        self.adaptive_weight = []
        
    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        self.KL_loss.append(logs.get('KL_loss'))
        self.recon_loss.append(logs.get('recon_loss'))
        self.adaptive_weight.append(logs.get('KL_loss')/(logs.get('recon_loss')+logs.get('KL_loss')))

# history = LossHistory()

optim = Adam(lr = learning_rate)
cvae.compile(optimizer=optim, loss=vae_loss, metrics = [KL_loss, recon_loss])
cvae_hist = cvae.fit([X_train, y_train], X_train, verbose = 1, batch_size=m, epochs=n_epoch)
# cvae_hist = cvae.fit([X_train, y_train], X_train, verbose = 1, batch_size=m, epochs=n_epoch, callbacks = [history])

# 利用学习的正态分布生成虚拟数据
z_train = encoder.predict([X_train, y_train])
encodings= np.asarray(z_train)
encodings = encodings.reshape(X_train.shape[0], n_z)

encodings_mean = np.mean(encodings,axis = 0)
encodings_cov = np.cov(encodings, rowvar = 0)

def generated_data(encodings_mean,encodings_cov,generate_num,i,labels_num = 4):
    eps = np.random.multivariate_normal(encodings_mean,encodings_cov,generate_num)
    y_label = np.zeros((generate_num,labels_num))
    y_label_add = np.ones((generate_num,1))
    y_label[:,i] = y_label[:,i] + y_label_add[:,0]
    return np.concatenate([eps,y_label],axis = 1)

Train_sample_num = [np.sum(Y_train==0),np.sum(Y_train==1),np.sum(Y_train==2),np.sum(Y_train==3)]
generate_max_num = max(Train_sample_num)+1
# generate_num = 40
labels_num = 4
for i in range(labels_num):
    generate_num = generate_max_num - Train_sample_num[i]
    k_data = generated_data(encodings_mean,encodings_cov,generate_num,i,labels_num)
    k_label = np.ones(generate_num*1) * i

    if i == 0:
        generated_array = k_data
        generated_label = k_label
    else:
        generated_array = np.concatenate([generated_array,k_data],axis = 0)
        generated_label = np.concatenate([generated_label,k_label],axis = 0)
        
generated_vitualdata = decoder.predict(generated_array)

kmm_method = KMM(kernel_type='rbf')
for i in range(labels_num):
    cvae_index = np.where(generated_label == i)
    real_index = np.where(Y_train == i)
    X_train_part = X_train[real_index]
    X_cvae_part = generated_vitualdata[cvae_index]
    beta_value = kmm_method.fit(X_cvae_part,X_train_part)
    print(i)
    if i == 0:
        beta_all = beta_value
    else:
        beta_all = np.concatenate([beta_all,beta_value],axis = 0)

X_real_train = np.concatenate([generated_vitualdata,X_train],axis = 0)
Y_real_train = np.concatenate([generated_label,Y_train],axis = 0)
Data_train = np.concatenate([X_real_train,Y_real_train.reshape(-1,1)],axis = 1) # X and Y
beta_ALL = np.concatenate([beta_all, np.ones([Y_train.shape[0],1])],axis = 0) # beta weight
sample_ALL = np.concatenate([np.zeros([beta_all.shape[0],1]), np.ones([Y_train.shape[0],1])],axis = 0) # real?

Data_Ours = np.concatenate([Data_train, beta_ALL, sample_ALL],axis = 1)
Data_Ours = pd.DataFrame(Data_Ours)
# Data_Ours.to_csv('Data_Ours_CVAE_imr4_1.csv', index = False)

Data_Ours_test = np.concatenate([X_test, Y_test.reshape(-1,1)],axis = 1)
Data_Ours_test = pd.DataFrame(Data_Ours_test)
# Data_Ours_test.to_csv('Data_Ours_CVAE_imr4_1_test.csv', index = False)

beta_value = beta_ALL
beta_value = beta_value.reshape(1,-1)
beta_value1 = beta_value[0]

F1_RF = 0
F1_CVAE_RF = 0
F1_MCVAE_RF = 0
Replication = 15
Results_matrix = np.ones([Replication,4])

for rs_i in range(Replication):
    rs_ii = 1 * rs_i + 99 # 定义随机种子
    model_RF_c = RandomForestClassifier(n_estimators=100,random_state = rs_ii)
    model_RF_c.fit(X_train,Y_train)

    Y_proba = model_RF_c.predict_proba(X_test)
    Y_pred = np.argmax(Y_proba, axis = 1)
    
    RF_accuracy = round(metrics.accuracy_score(Y_test, Y_pred),4)
    RF_c_recall = round(metrics.recall_score(Y_test, Y_pred, average = "macro"),4)
    RF_c_precision = round(metrics.precision_score(Y_test, Y_pred, average = "macro"),4)
    RF_c_f1_score = round(metrics.f1_score(Y_test, Y_pred, average='macro'),4)
    print('N-Real-RF',rs_ii,RF_accuracy, RF_c_recall,RF_c_precision,RF_c_f1_score)
    Results_matrix[rs_i] = [RF_accuracy, RF_c_recall,RF_c_precision,RF_c_f1_score]
    
    F1_RF = F1_RF + RF_c_f1_score

# CAVE-RF
for rs_i in range(Replication):
    rs_ii = 1 * rs_i + 99
    model_CAVE_RF = RandomForestClassifier(n_estimators=100,random_state = rs_ii)
    model_CAVE_RF.fit(X_real_train,Y_real_train)

    Y_proba = model_CAVE_RF.predict_proba(X_test)
    Y_pred = np.argmax(Y_proba, axis = 1)
    
    CAVE_RF_accuracy = round(metrics.accuracy_score(Y_test, Y_pred),4)
    CAVE_RF_recall = round(metrics.recall_score(Y_test, Y_pred, average = "macro"),4)
    CAVE_RF_precision = round(metrics.precision_score(Y_test, Y_pred, average = "macro"),4)
    CAVE_RF_f1_score = round(metrics.f1_score(Y_test, Y_pred, average='macro'),4)
    print('W-CAVE-RF',rs_ii, CAVE_RF_accuracy, CAVE_RF_recall, CAVE_RF_precision, CAVE_RF_f1_score)
    
    F1_CVAE_RF = F1_CVAE_RF + CAVE_RF_f1_score
# WM-CAVE-RF
for rs_i in range(Replication):
    rs_ii = 1 * rs_i + 99
    model_CAVE_KMM_RF = RandomForestClassifier(n_estimators=100,random_state = rs_ii)
    model_CAVE_KMM_RF.fit(X_real_train,Y_real_train,sample_weight = beta_value1)

    Y_proba = model_CAVE_KMM_RF.predict_proba(X_test)
    Y_pred = np.argmax(Y_proba, axis = 1)

    CAVE_RF_accuracy = round(metrics.accuracy_score(Y_test, Y_pred),4)
    CAVE_RF_recall = round(metrics.recall_score(Y_test, Y_pred, average = "macro"),4)
    CAVE_RF_precision = round(metrics.precision_score(Y_test, Y_pred, average = "macro"),4)
    CAVE_RF_f1_score = round(metrics.f1_score(Y_test, Y_pred, average='macro'),4)
    print('WMCVAE-RF',rs_ii, CAVE_RF_accuracy, CAVE_RF_recall, CAVE_RF_precision, CAVE_RF_f1_score)
    
    F1_MCVAE_RF = F1_MCVAE_RF + CAVE_RF_f1_score
    
print(F1_RF/Replication,F1_CVAE_RF/Replication,F1_MCVAE_RF/Replication)
