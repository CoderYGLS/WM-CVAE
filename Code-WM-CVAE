# CVAE 
# Import python package
import keras
import tensorflow as tf
import os
import warnings
import pandas as pd
import numpy as np

import time
from tensorflow.keras.layers import Dense
from keras.layers import Input, Lambda
from keras.layers import concatenate as concat
from keras.models import Model
from keras import backend as K
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam

# from scipy.misc import imsave
import imageio
imsave = imageio.imsave
import matplotlib.pyplot as plt
plt.rc('font',family='Times New Roman')
plt.rcParams['figure.dpi'] = 300
warnings.filterwarnings('ignore')
%pylab inline

from cvxopt import matrix, solvers

import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import train_test_split

from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

def seed_tensorflow(seed = 42):
    os.environ['PYTHONHASHSEED'] = str(seed) 
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1' # `pip install tensorflow-determinism` first,使用与tf>2.1
our_seed = 2020
seed_tensorflow(our_seed)

# Kernel Mean Matching 
def kernel(ker, X1, X2, gamma):
    K = None
    if ker == 'linear':
        if X2 is not None:
            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1), np.asarray(X2))
        else:
            K = sklearn.metrics.pairwise.linear_kernel(np.asarray(X1))
    elif ker == 'rbf':
        if X2 is not None:
            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1), np.asarray(X2), gamma)
        else:
            K = sklearn.metrics.pairwise.rbf_kernel(np.asarray(X1), None, gamma)
    return K

class KMM:
    def __init__(self, kernel_type='linear', gamma=1.0, B=1.0, eps=None):
        '''
        Initialization function
        :param kernel_type: 'linear' | 'rbf'
        :param gamma: kernel bandwidth for rbf kernel
        :param B: bound for beta
        :param eps: bound for sigma_beta
        '''
        self.kernel_type = kernel_type
        self.gamma = gamma
        self.B = B
        self.eps = eps

    def fit(self, Xs, Xt):
        '''
        Fit source and target using KMM (compute the coefficients)
        :param Xs: ns * dim
        :param Xt: nt * dim
        :return: Coefficients (Pt / Ps) value vector (Beta in the paper)
        '''
        ns = Xs.shape[0]
        nt = Xt.shape[0]
        if self.eps == None:
            self.eps = self.B / np.sqrt(ns)
        K = kernel(self.kernel_type, Xs, None, self.gamma)
        kappa = np.sum(kernel(self.kernel_type, Xs, Xt, self.gamma) * float(ns) / float(nt), axis=1)

        K = matrix(K.astype(np.double))
        kappa = matrix(kappa.astype(np.double))
        G = matrix(np.r_[np.ones((1, ns)), -np.ones((1, ns)), np.eye(ns), -np.eye(ns)])
        h = matrix(np.r_[ns * (1 + self.eps), ns * (self.eps - 1), self.B * np.ones((ns,)), np.zeros((ns,))])

        sol = solvers.qp(K, -kappa, G, h)
        beta = np.array(sol['x'])
        return beta
# We can get the X_train, Y_train, X_test, Y_test, and y_train, y_test = to_categorical(Y_train), to_categorical(Y_test) here.
# Hyper-parameters settings
n_z = 8 # latent space size
encoder_dim1 = 128 # dim of encoder hidden layer
encoder_dim2 = 32
decoder_dim1 = 32 # dim of decoder hidden layer
decoder_dim2 = 128
decoder_out_dim = 27 # dim of decoder output layer
activ = 'relu'
m = 32
learning_rate = 0.0005
n_epoch = 100

n_x = X_train.shape[1]
n_y = y_train.shape[1]

# Encoder settings
X = Input(shape=(n_x,))
label = Input(shape=(n_y,))
inputs = concat([X, label])
encoder_h1 = Dense(encoder_dim1, activation=activ, name = 'encoder_first')(inputs)
encoder_h2 = Dense(encoder_dim2, activation=activ, name = 'encoder_second')(encoder_h1)
mu = Dense(n_z, activation='linear', name = 'encoder_out_mu')(encoder_h2)
l_sigma = Dense(n_z, activation='linear', name = 'encoder_out_sigma')(encoder_h2)
def sample_z(args):
    mu, l_sigma = args
    mu_num = tf.shape(mu)[0]
    eps = K.random_normal(shape=(mu_num, n_z), mean=0., stddev=1.)
    return mu + K.exp(l_sigma / 2) * eps
# Sampling latent space
z = Lambda(sample_z, output_shape = (n_z, ), name = 'lambda_layer')([mu, l_sigma])
# Merge latent space with label
zc = concat([z, label])

# Decoder settings
decoder_hidden1 = Dense(decoder_dim1, activation=activ)
decoder_hidden2 = Dense(decoder_dim2, activation=activ)
decoder_out = Dense(decoder_out_dim, activation='tanh')
h_p1 = decoder_hidden1(zc)
h_p2 = decoder_hidden2(h_p1)
outputs = decoder_out(h_p2)

# define loss
def vae_loss(y_true, y_pred):
    recon = K.sum(K.abs(y_true-y_pred), axis=-1)
    kl = 0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=-1)
    return recon + kl/(kl + recon) * kl 
#     return recon +  kl (commomn format)

def KL_loss(y_true, y_pred):
    return(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))

def recon_loss(y_true, y_pred):
    return K.sum(K.abs(y_true-y_pred), axis=-1)

# CVAE-model build
cvae = Model(inputs=[X, label], outputs = outputs)
encoder = Model(inputs=[X, label],outputs = mu)

d_in = Input(shape=(n_z+n_y,))
d_h1 = decoder_hidden1(d_in)
d_h2 = decoder_hidden2(d_h1)
d_out = decoder_out(d_h2)
decoder = Model(inputs= d_in, outputs = d_out)

class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.KL_loss = []
        self.recon_loss = []
        self.adaptive_weight = []
        
    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        self.KL_loss.append(logs.get('KL_loss'))
        self.recon_loss.append(logs.get('recon_loss'))
        self.adaptive_weight.append(logs.get('KL_loss')/(logs.get('recon_loss')+logs.get('KL_loss')))

history = LossHistory()
start_time = time.time()
optim = Adam(lr = learning_rate)
cvae.compile(optimizer=optim, loss=vae_loss, metrics = [KL_loss, recon_loss])
cvae_hist = cvae.fit([X_train, y_train], X_train, verbose = 1, batch_size=m, epochs=n_epoch, callbacks = [history])
end_time = time.time()
process_time = end_time - start_time
print(process_time)


# Generate virtual data by normal distributions
z_train = encoder.predict([X_train, y_train])
encodings= np.asarray(z_train)
encodings = encodings.reshape(X_train.shape[0], n_z)

encodings_mean = np.mean(encodings,axis = 0)
encodings_cov = np.cov(encodings, rowvar = 0)

def generated_data(encodings_mean,encodings_cov,generate_num,i,labels_num = 7):
    eps = np.random.multivariate_normal(encodings_mean,encodings_cov,generate_num)
    y_label = np.zeros((generate_num,labels_num))
    y_label_add = np.ones((generate_num,1))
    y_label[:,i] = y_label[:,i] + y_label_add[:,0]
    return np.concatenate([eps,y_label],axis = 1)

Train_sample_num = [np.sum(Y_train==0),np.sum(Y_train==1),np.sum(Y_train==2),np.sum(Y_train==3),
                    np.sum(Y_train==4),np.sum(Y_train==5),np.sum(Y_train==6)]
generate_max_num = max(Train_sample_num)+1
# generate_num = 40
labels_num = 7
for i in range(labels_num):
    generate_num = generate_max_num - Train_sample_num[i]
    k_data = generated_data(encodings_mean,encodings_cov,generate_num,i,labels_num)
    k_label = np.ones(generate_num*1) * i

    if i == 0:
        generated_array = k_data
        generated_label = k_label
    else:
        generated_array = np.concatenate([generated_array,k_data],axis = 0)
        generated_label = np.concatenate([generated_label,k_label],axis = 0)
        
generated_vitualdata = decoder.predict(generated_array)

kmm_method = KMM(kernel_type='rbf')
for i in range(labels_num):
    cvae_index = np.where(generated_label == i)
    real_index = np.where(Y_train == i)
    X_train_part = X_train[real_index]
    X_cvae_part = generated_vitualdata[cvae_index]
    beta_value = kmm_method.fit(X_cvae_part,X_train_part)
    print(i)
    if i == 0:
        beta_all = beta_value
    else:
        beta_all = np.concatenate([beta_all,beta_value],axis = 0)

X_real_train = np.concatenate([generated_vitualdata,X_train],axis = 0)
Y_real_train = np.concatenate([generated_label,Y_train],axis = 0)
Data_train = np.concatenate([X_real_train,Y_real_train.reshape(-1,1)],axis = 1) # X and Y
beta_ALL = np.concatenate([beta_all, np.ones([Y_train.shape[0],1])],axis = 0) # beta weight
sample_ALL = np.concatenate([np.zeros([beta_all.shape[0],1]), np.ones([Y_train.shape[0],1])],axis = 0) # real?

Data_Ours = np.concatenate([Data_train, beta_ALL, sample_ALL],axis = 1)
Data_Ours = pd.DataFrame(Data_Ours)
# Data_Ours.to_csv('Data_Ours_CVAE_imr4_1.csv', index = False)

Data_Ours_test = np.concatenate([X_test, Y_test.reshape(-1,1)],axis = 1)
Data_Ours_test = pd.DataFrame(Data_Ours_test)
# Data_Ours_test.to_csv('Data_Ours_CVAE_imr4_1_test.csv', index = False)

beta_value = beta_ALL
beta_value = beta_value.reshape(1,-1)
beta_value1 = beta_value[0]

X_real_train = np.concatenate([generated_vitualdata,X_train],axis = 0)
Y_real_train = np.concatenate([generated_label,Y_train],axis = 0)
Data_train = np.concatenate([X_real_train,Y_real_train.reshape(-1,1)],axis = 1) # X and Y
beta_ALL = np.concatenate([beta_all, np.ones([Y_train.shape[0],1])],axis = 0) # beta weight
sample_ALL = np.concatenate([np.zeros([beta_all.shape[0],1]), np.ones([Y_train.shape[0],1])],axis = 0) # real?

Data_Ours = np.concatenate([Data_train, beta_ALL, sample_ALL],axis = 1)
Data_Ours = pd.DataFrame(Data_Ours)
Data_Ours.to_csv('Data_Ours_CVAE_imr10.csv', index = False)

Data_Ours_test = np.concatenate([X_test, Y_test.reshape(-1,1)],axis = 1)
Data_Ours_test = pd.DataFrame(Data_Ours_test)
Data_Ours_test.to_csv('Data_Ours_CVAE_imr10_test.csv', index = False)

F1_RF = 0
F1_CVAE_RF = 0
F1_MCVAE_RF = 0
Replication = 30
for rs_i in range(Replication):
#     rs_ii = 5 * rs_i + 0
    rs_ii = 1 * rs_i + 10
    model_RF_c = RandomForestClassifier(n_estimators=100,random_state = rs_ii)
    model_RF_c.fit(X_train,Y_train)

    Y_proba = model_RF_c.predict_proba(X_test)
    Y_pred = np.argmax(Y_proba, axis = 1)
    
    RF_accuracy = round(metrics.accuracy_score(Y_test, Y_pred),4)
    RF_c_recall = round(metrics.recall_score(Y_test, Y_pred, average = "macro"),4)
    RF_c_precision = round(metrics.precision_score(Y_test, Y_pred, average = "macro"),4)
    RF_c_f1_score = round(metrics.f1_score(Y_test, Y_pred, average='macro'),4)
    print('N-Real-RF',rs_ii,RF_accuracy, RF_c_recall,RF_c_precision,RF_c_f1_score)
    F1_RF = F1_RF + RF_c_f1_score

# CAVE-RF
for rs_i in range(Replication):
    rs_ii = 1 * rs_i + 10
    model_CAVE_RF = RandomForestClassifier(n_estimators=100,random_state = rs_ii)
    model_CAVE_RF.fit(X_real_train,Y_real_train)

    Y_proba = model_CAVE_RF.predict_proba(X_test)
    Y_pred = np.argmax(Y_proba, axis = 1)
    
    CAVE_RF_accuracy = round(metrics.accuracy_score(Y_test, Y_pred),4)
    CAVE_RF_recall = round(metrics.recall_score(Y_test, Y_pred, average = "macro"),4)
    CAVE_RF_precision = round(metrics.precision_score(Y_test, Y_pred, average = "macro"),4)
    CAVE_RF_f1_score = round(metrics.f1_score(Y_test, Y_pred, average='macro'),4)
    print('W-CAVE-RF',rs_ii, CAVE_RF_accuracy, CAVE_RF_recall, CAVE_RF_precision, CAVE_RF_f1_score)
    
    F1_CVAE_RF = F1_CVAE_RF + CAVE_RF_f1_score
# WM-CAVE-RF
for rs_i in range(Replication):
    rs_ii = 1 * rs_i + 1000
    model_CAVE_KMM_RF = RandomForestClassifier(n_estimators=100,random_state = rs_ii)
    model_CAVE_KMM_RF.fit(X_real_train,Y_real_train,sample_weight = beta_value1)

    Y_proba = model_CAVE_KMM_RF.predict_proba(X_test)
    Y_pred = np.argmax(Y_proba, axis = 1)

    CAVE_RF_accuracy = round(metrics.accuracy_score(Y_test, Y_pred),4)
    CAVE_RF_recall = round(metrics.recall_score(Y_test, Y_pred, average = "macro"),4)
    CAVE_RF_precision = round(metrics.precision_score(Y_test, Y_pred, average = "macro"),4)
    CAVE_RF_f1_score = round(metrics.f1_score(Y_test, Y_pred, average='macro'),4)
    print('WMCVAE-RF',rs_ii, CAVE_RF_accuracy, CAVE_RF_recall, CAVE_RF_precision, CAVE_RF_f1_score)
    
    F1_MCVAE_RF = F1_MCVAE_RF + CAVE_RF_f1_score
    
print(F1_RF/Replication,F1_CVAE_RF/Replication,F1_MCVAE_RF/Replication)

rs_ii = 22
model_CAVE_KMM_RF = RandomForestClassifier(n_estimators=100,random_state = rs_ii)
model_CAVE_KMM_RF.fit(X_real_train,Y_real_train,sample_weight = beta_value1)

Y_proba = model_CAVE_KMM_RF.predict_proba(X_test)
Y_pred = np.argmax(Y_proba, axis = 1)

CAVE_RF_accuracy = round(metrics.accuracy_score(Y_test, Y_pred),4)
CAVE_RF_recall = round(metrics.recall_score(Y_test, Y_pred, average = "macro"),4)
CAVE_RF_precision = round(metrics.precision_score(Y_test, Y_pred, average = "macro"),4)
CAVE_RF_f1_score = round(metrics.f1_score(Y_test, Y_pred, average='macro'),4)
print('WMCVAE-RF',rs_ii, CAVE_RF_accuracy, CAVE_RF_recall, CAVE_RF_precision, CAVE_RF_f1_score)
    

confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print(confusion_matrix)

# Apply the conventional data augmentation methods as a comparison using the imlbearn library
from imblearn.over_sampling import RandomOverSampler 
from imblearn.over_sampling import SMOTE, ADASYN
r_s = 200
over = RandomOverSampler() 
X_Over, Y_Over = over.fit_resample(X_train, Y_train)

model_RF_c = RandomForestClassifier(n_estimators=50,random_state = r_s)
model_RF_c.fit(X_Over, Y_Over)

Y_proba = model_RF_c.predict_proba(X_test)
Y_pred = np.argmax(Y_proba, axis = 1)

RF_c_recall = metrics.recall_score(Y_test, Y_pred, average = "macro")
RF_c_precision = metrics.precision_score(Y_test, Y_pred, average = "macro")
RF_c_f1_score = metrics.f1_score(Y_test, Y_pred, average='macro')
print('Over sampling - RF',RF_c_recall,RF_c_precision,RF_c_f1_score)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print(confusion_matrix)

from imblearn.under_sampling import RandomUnderSampler 
Under = RandomUnderSampler() 
X_Under, Y_Under = Under.fit_resample(X_train, Y_train)
# Real - RF
model_RF_c = RandomForestClassifier(n_estimators=50,random_state = r_s)
model_RF_c.fit(X_Under, Y_Under)

Y_proba = model_RF_c.predict_proba(X_test)
Y_pred = np.argmax(Y_proba, axis = 1)

RF_c_recall = metrics.recall_score(Y_test, Y_pred, average = "macro")
RF_c_precision = metrics.precision_score(Y_test, Y_pred, average = "macro")
RF_c_f1_score = metrics.f1_score(Y_test, Y_pred, average='macro')
print('Under sampling - RF',RF_c_recall,RF_c_precision,RF_c_f1_score)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print(confusion_matrix)

X_SMOTE, Y_SMOTE = SMOTE(k_neighbors=2).fit_resample(X_train, Y_train)
# Real - RF
model_RF_c = RandomForestClassifier(n_estimators=50,random_state = r_s)
model_RF_c.fit(X_SMOTE, Y_SMOTE)

Y_proba = model_RF_c.predict_proba(X_test)
Y_pred = np.argmax(Y_proba, axis = 1)

RF_c_recall = metrics.recall_score(Y_test, Y_pred, average = "macro")
RF_c_precision = metrics.precision_score(Y_test, Y_pred, average = "macro")
RF_c_f1_score = metrics.f1_score(Y_test, Y_pred, average='macro')
print('SMOTE - RF',RF_c_recall,RF_c_precision,RF_c_f1_score)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print(confusion_matrix)

X_ADASYN, Y_ADASYN = ADASYN(n_neighbors=2).fit_resample(X_train, Y_train)

model_RF_c = RandomForestClassifier(n_estimators=50,random_state = r_s)
model_RF_c.fit(X_ADASYN, Y_ADASYN)

Y_proba = model_RF_c.predict_proba(X_test)
Y_pred = np.argmax(Y_proba, axis = 1)

RF_c_recall = metrics.recall_score(Y_test, Y_pred, average = "macro")
RF_c_precision = metrics.precision_score(Y_test, Y_pred, average = "macro")
RF_c_f1_score = metrics.f1_score(Y_test, Y_pred, average='macro')
print('ADASYN - RF',RF_c_recall,RF_c_precision,RF_c_f1_score)
confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print(confusion_matrix)
